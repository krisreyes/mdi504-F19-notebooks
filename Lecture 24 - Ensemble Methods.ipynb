{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----- make nice figures -----\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi']= 200\n",
    "from cycler import cycler\n",
    "COLORS = ['#F00D2C', '#242482', '#0071BE', '#4E8F00', '#553C67', '#DA5319']\n",
    "default_cycler = cycler(color=COLORS)\n",
    "plt.rc('axes', prop_cycle=default_cycler) \n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('data/perovskite_data.txt')\n",
    "\n",
    "# Shuffle data\n",
    "I_perm = np.random.permutation(len(data))\n",
    "data = data[I_perm, :]\n",
    "\n",
    "# Class data - whether we form Perovskite or not\n",
    "c = data[:, 0]\n",
    "\n",
    "# Extract all but the 0-th column\n",
    "X = data[:, 1:]\n",
    "\n",
    "# Normalize data\n",
    "X = (X - np.mean(X, axis=0))/np.std(X, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training, validation and testing set\n",
    "num_points = len(c)\n",
    "num_train = int(0.8*0.8*num_points)\n",
    "num_valid = int(0.8*0.2*num_points)\n",
    "num_test = len(c) - num_train - num_valid\n",
    "\n",
    "X_train = X[:num_train, :]\n",
    "c_train = c[:num_train]\n",
    "\n",
    "X_valid = X[num_train:(num_train + num_valid), :]\n",
    "c_valid = c[num_train:(num_train + num_valid)]\n",
    "\n",
    "# get the last \"num_test\" rows\n",
    "X_test = X[-num_test:, :]\n",
    "c_test = c[-num_test:]\n",
    "\n",
    "print(\"         Total data size: \" + str(num_points))\n",
    "print(\"  Training data set size: \" + str(num_train))\n",
    "print(\"Validation data set size: \" + str(num_valid))\n",
    "print(\"   Testing data set size: \" + str(num_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "model = BaggingClassifier(n_estimators = 3, max_samples = 0.1)\n",
    "model.fit(X_train, c_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score\n",
    "\n",
    "# check with testing data\n",
    "c_valid_model = model.predict(X_valid)\n",
    "\n",
    "# Precision, recall confusion matrix\n",
    "precision = precision_score(c_valid, c_valid_model)\n",
    "recall = recall_score(c_valid, c_valid_model)\n",
    "acc = accuracy_score(c_valid, c_valid_model)\n",
    "f1 = f1_score(c_valid, c_valid_model)\n",
    "\n",
    "print(\"Bagging:\")\n",
    "print(\"Precision = \" + str(precision))\n",
    "print(\"   Recall = \" + str(recall))\n",
    "print(\" Accuracy = \" + str(acc))\n",
    "print(\"       F1 = \" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = range(1, 10)\n",
    "max_samples = np.linspace(0.05, 1.0, 10)\n",
    "accuracies = np.zeros([len(n_estimators), len(max_samples)])\n",
    "\n",
    "for i, n in enumerate(n_estimators):\n",
    "    for j, s in enumerate(max_samples):\n",
    "        model = BaggingClassifier(n_estimators = n, max_samples = s)\n",
    "        model.fit(X_train, c_train)\n",
    "        c_valid_model = model.predict(X_valid)\n",
    "        accuracies[i,j] = accuracy_score(c_valid, c_valid_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flat index of best accuracy\n",
    "ij_max = np.argmax(accuracies)\n",
    "print(ij_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back into (i,j) pair\n",
    "ij_max = np.unravel_index(ij_max, accuracies.shape)\n",
    "print(ij_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_n = n_estimators[ij_max[0]]\n",
    "best_s = max_samples[ij_max[1]]\n",
    "\n",
    "model = BaggingClassifier(n_estimators = best_n, max_samples = best_s)\n",
    "model.fit(X_train, c_train)\n",
    "c_valid_model = model.predict(X_valid)\n",
    "\n",
    "# score best model\n",
    "precision = precision_score(c_valid, c_valid_model)\n",
    "recall = recall_score(c_valid, c_valid_model)\n",
    "acc = accuracy_score(c_valid, c_valid_model)\n",
    "f1 = f1_score(c_valid, c_valid_model)\n",
    "\n",
    "print(\"Bagging:\")\n",
    "print(\"Precision = \" + str(precision))\n",
    "print(\"   Recall = \" + str(recall))\n",
    "print(\" Accuracy = \" + str(acc))\n",
    "print(\"       F1 = \" + str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators = 3)\n",
    "model.fit(X_train, c_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check with testing data\n",
    "c_valid_model = model.predict(X_valid)\n",
    "\n",
    "# Precision, recall confusion matrix\n",
    "precision = precision_score(c_valid, c_valid_model)\n",
    "recall = recall_score(c_valid, c_valid_model)\n",
    "acc = accuracy_score(c_valid, c_valid_model)\n",
    "f1 = f1_score(c_valid, c_valid_model)\n",
    "\n",
    "print(\"Random Forest:\")\n",
    "print(\"Precision = \" + str(precision))\n",
    "print(\"   Recall = \" + str(recall))\n",
    "print(\" Accuracy = \" + str(acc))\n",
    "print(\"       F1 = \" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "\n",
    "# Plot the features importances using a bar chart\n",
    "feature_names = [\"A-EN - O-EN\", \"rA/rO\", \"A-ionicity\", \"dA-O\", \"B-EN - O-EN\", \"rB/rO\", \"B-ionicity\", \"dB-O\", \"dA-O/dB-O\", \"A-EN - B-EN\", \"rA/rB\", \"t_BV\", \"t_IR\", \"GII\"]\n",
    "x_plot = [2*i for i in range(len(importances))]\n",
    "plt.barh(x_plot, model.feature_importances_)\n",
    "plt.yticks(x_plot, feature_names)\n",
    "plt.grid(axis = 'x')\n",
    "plt.xlabel('Importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model = AdaBoostClassifier()\n",
    "model.fit(X_train, c_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check with testing data\n",
    "c_valid_model = model.predict(X_valid)\n",
    "\n",
    "# Precision, recall confusion matrix\n",
    "precision = precision_score(c_valid, c_valid_model)\n",
    "recall = recall_score(c_valid, c_valid_model)\n",
    "acc = accuracy_score(c_valid, c_valid_model)\n",
    "f1 = f1_score(c_valid, c_valid_model)\n",
    "\n",
    "print(\"Adaboost:\")\n",
    "print(\"Precision = \" + str(precision))\n",
    "print(\"   Recall = \" + str(recall))\n",
    "print(\" Accuracy = \" + str(acc))\n",
    "print(\"       F1 = \" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the individual classifiers\n",
    "individual_models = model.estimators_\n",
    "\n",
    "# individual model predictions\n",
    "individual_acc = np.zeros(len(individual_models))\n",
    "for i, model_i in enumerate(individual_models):\n",
    "    # check with testing data\n",
    "    c_valid_model = model_i.predict(X_valid)\n",
    "    \n",
    "    # convert to -1\n",
    "    c_valid_model[c_valid_model == 0] = -1\n",
    "    \n",
    "    # accuracy\n",
    "    individual_acc[i] = accuracy_score(c_valid, c_valid_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(individual_acc, rwidth=0.9)\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Adaboost: individual model accuracies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(individual_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to random forest\n",
    "model = BaggingClassifier(n_estimators = 50, max_samples = best_s)\n",
    "model.fit(X_train, c_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the individual classifiers\n",
    "individual_models = model.estimators_\n",
    "\n",
    "# individual model predictions\n",
    "individual_acc = np.zeros(len(individual_models))\n",
    "for i, model_i in enumerate(individual_models):\n",
    "    # check with testing data\n",
    "    c_valid_model = model_i.predict(X_valid)\n",
    "    \n",
    "    c_valid_model[c_valid_model == 0] = -1\n",
    "    \n",
    "    # accuracy\n",
    "    individual_acc[i] = accuracy_score(c_valid, c_valid_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(individual_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(individual_acc, rwidth=0.9)\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Bagging: individual model accuracies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
