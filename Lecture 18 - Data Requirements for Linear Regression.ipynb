{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----- make nice figures -----\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi']= 200\n",
    "from cycler import cycler\n",
    "COLORS = ['#F00D2C', '#242482', '#0071BE', '#4E8F00', '#553C67', '#DA5319']\n",
    "default_cycler = cycler(color=COLORS)\n",
    "plt.rc('axes', prop_cycle=default_cycler) \n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data requirements in 1 dimension is misleading\n",
    "\n",
    "If we only have 1 material descriptor, i.e. if $x$ is 1 dimensional, then the data requirements to fit most reasonable functions is actually quite low. To put it another way, for a given set of data, we can actually fit a fairly high degree polynomial, which is often enough for most settings, even if the truth isn't polynomial.\n",
    "\n",
    "Below, we generate noisy data from the function\n",
    "\n",
    "$$y = \\sin(5x) - 0.3\\sin(20x) + 2.4x,$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(0, 1, 60)\n",
    "y = np.sin(5*x) - 0.3*np.sin(20*x) + 2.4*x  + np.random.normal(0, 0.1, len(x))\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming this is all training data (i.e. we have already removed the data needed for validation and testing), then we can fit anywhere between a degree 5 to a degree 29 polynomial and stay within the $2-10\\times$ rule of thumb between data and number of basis functions. Let's try to fit it to a 15 degree polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 15\n",
    "\n",
    "# Solve least squares solution\n",
    "Phi = np.vander(x, d+1)\n",
    "theta_ls = np.linalg.lstsq(Phi, y)[0]\n",
    "\n",
    "# plot model and the data\n",
    "x_plot = np.linspace(0, 1, 100)\n",
    "Phi_plot = np.vander(x_plot, d+1)\n",
    "y_plot = Phi_plot @ theta_ls\n",
    "plt.plot(x_plot, y_plot, color = COLORS[1])\n",
    "plt.scatter(x, y)\n",
    "plt.legend(['Model', 'Data'])\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a pretty good fit dispite the function not actually being a degree 15 polynomial. In fact, we can plot the noiseless truth as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_truth = np.sin(5*x_plot) - 0.3*np.sin(20*x_plot) + 2.4*x_plot\n",
    "plt.plot(x_plot, y_truth, color = COLORS[3])\n",
    "plt.plot(x_plot, y_plot, color = COLORS[1])\n",
    "plt.scatter(x, y)\n",
    "plt.legend(['Truth', 'Model', 'Data'])\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the number of monomials of a certain degree.\n",
    "\n",
    "To see how this is misleading, let us instead consider how many monomials in $k$ variables of degree at most $n$. This number will be the size of the basis function set, and ultimately determine our data requirements if we wish to use the monomial basis and still follow the $2-10\\times$ rule of thumb.\n",
    "\n",
    "The number of such monomials is given by the combinatorial formula\n",
    "\n",
    "$$\\binom{n + k}{n} - 1.$$\n",
    "\n",
    "We plot this for different values of $n$ and $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import binom\n",
    "\n",
    "n = 4\n",
    "k = np.arange(1, 15).astype('int')\n",
    "num_monoms = binom(n+k, n) - 1\n",
    "\n",
    "plt.plot(k, num_monoms)\n",
    "plt.grid()\n",
    "plt.xlabel('Number of descriptors (k)')\n",
    "plt.ylabel('Number of monomials')\n",
    "plt.title('n = %d' % n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This combinatorial growth in our data requirements as we increase k is a manifestation of the **Curse of Dimensionality**, which in broad terms is just the observation that problems quickly become difficult as their dimensionaltiy increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least squares when we don't have enough data\n",
    "\n",
    "In high dimensions, therefore, we are often in a data-deficient setting. Below, we consider the case where we only have 5 data points, and we're trying to fit to a degree 20 polynomial. This is bound to fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(0, 1, 5)\n",
    "y = 1 + 10*x - 100*x**2 + 1000*x**3 + np.random.normal(0, 0.2, len(x))\n",
    "Phi = np.vander(x, 20, increasing=True)\n",
    "print(Phi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that if we were to try to find the least-squares solution to this, we'd have to solve the normal equations\n",
    "\n",
    "$$\\Phi^T \\Phi \\vec \\theta = \\Phi^T \\vec y$$\n",
    "\n",
    "In this setting, the matrix on the left-hand side $A = \\Phi^T\\Phi$ is **rank deficient,$$ which means a unique solution to the above is impossible. Below we see that despite being a $20\\times 20$ matrix, its rank is only $5$ (this is the most it can be since we had 5 data points).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Phi.T@Phi\n",
    "rank = np.linalg.matrix_rank(A)\n",
    "\n",
    "print(\"Size of A = \" + str(A.shape))\n",
    "print(\"Rank of A = \" + str(rank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, there are an infinite number of solutions to this system. I do not know which one the `numpy.linalg.lstsq` returns. Regardless, we can try to visualize the solution, and in particular the size of its entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_ls = np.linalg.lstsq(Phi, y, rcond=None)[0]\n",
    "plt.plot(theta_ls, marker = 'o' )\n",
    "plt.grid()\n",
    "plt.xlabel('i')\n",
    "plt.ylabel('theta_i')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the size of some of its entries can get really large. This is indicative of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motivated by above, we would like to restrict the space of least-square solutions $\\vec \\theta$, which minimize the least-squares loss function\n",
    "\n",
    "$$ \\large L_\\text{LS}(\\vec \\theta) = \\| \\vec y - \\Phi \\vec \\theta \\|^2_2,$$\n",
    "\n",
    "where $\\Phi$ is the $m \\times n$ design matrix, $\\vec y$ is the observation vector and $\\vec \\theta$ is the vector of coefficients we are trying to solve for. The least-squares solution is the $\\vec \\theta$ that minimizes this:\n",
    "\n",
    "$$ \\large \\vec\\theta_\\text{LS} = \\arg\\min L_\\text{LS}(\\vec \\theta).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two regularization techniques:\n",
    "* L1 regularization (aka **Lasso**):\n",
    "$$\\large L_\\text{Lasso}(\\vec \\theta) = \\| \\vec y - \\Phi \\vec \\theta \\|_2^2 + \\lambda \\| \\vec \\theta\\|_1.\\\\ $$\n",
    "\n",
    "* L2 regularization (aka **Ridge (or Tikhonov) Regularization**):\n",
    "$$\\large L_\\text{Ridge}(\\vec \\theta) = \\| \\vec y - \\Phi \\vec \\theta \\|_2 ^2 + \\lambda \\| \\vec \\theta\\|_2^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we'll plot some noisy data and the true function it came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 1 + 10*x - 50*x**2 +1000*x**3 - 1000*x**4\n",
    "\n",
    "x_data = np.linspace(0, 1, 10)\n",
    "y_data = f(x_data) + np.random.normal(0, 10, len(x_data))\n",
    "plt.scatter(x_data, y_data, color='r')\n",
    "\n",
    "x_plot = np.linspace(0, 1, 100)\n",
    "y_truth = f(x_plot)\n",
    "plt.plot(x_plot, y_truth, color = COLORS[3])\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "plt.legend(['Data', 'Truth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try to fit a degree 10 polynomial, which is too large of a model for our limited set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi = np.vander(x, 11, increasing=True)\n",
    "\n",
    "# Print out rank for our information\n",
    "rank = np.linalg.matrix_rank(Phi)\n",
    "num_cols = Phi.shape[1]\n",
    "print(\"Phi has rank = \" + str(rank))\n",
    "print(\"n = \" + str(num_cols))\n",
    "\n",
    "# Get least squares solution\n",
    "theta_ls = np.linalg.lstsq(Phi, y_obs, rcond=None)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate and plot the model\n",
    "Phi_plot = np.vander(x_plot, 11, increasing=True)\n",
    "y_model = Phi_plot @ theta_ls\n",
    "plt.plot(x_plot, y_model, color = COLORS[1])\n",
    "\n",
    "# Also plot the truth\n",
    "plt.plot(x_plot, y_truth,  color = COLORS[3])\n",
    "\n",
    "# and plot the data\n",
    "plt.scatter(x, y_obs,)\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(['Least-squares fit', 'Truth',  'Data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we feared, it overfit the data. Let's look at the entries of the least-square solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What theta values do we get?\n",
    "plt.plot(theta_ls, linewidth=0, marker='o', markersize=5)\n",
    "plt.xlabel('i')\n",
    "plt.ylabel('theta_i')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to control or penalize large values of theta to avoid this type of behavior. So instead of minimizing least squares loss, we'll introduce a penalty term for large coefficient values:\n",
    "\n",
    "$$\\large L_\\text{Ridge}(\\vec \\theta) = \\| \\vec y - \\Phi \\vec \\theta \\|_2 ^2 + \\lambda \\| \\vec \\theta\\|_2^2 $$\n",
    "\n",
    "We'll use the library: `sklearn.linear_model.Ridge` for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# 1. set up model. Here alpha is what we've been calling lambda above\n",
    "ridge_model = Ridge(alpha = 1.0)\n",
    "\n",
    "# 2. Fit the model\n",
    "ridge_model.fit(Phi, y_obs)\n",
    "\n",
    "# 3. Get coefficients\n",
    "theta_ridge = ridge_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot the ridge regression solution, we see that the values are much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(theta_ridge, linewidth=0, marker='o', markersize=5)\n",
    "plt.xlabel('i')\n",
    "plt.ylabel('theta_i')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the model vs. truth and data again to see our fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_model = Phi_plot @ theta_ridge\n",
    "plt.plot(x_plot, y_model, color = COLORS[1])\n",
    "\n",
    "# Also plot the truth\n",
    "plt.plot(x_plot, y_truth,  color = COLORS[3])\n",
    "\n",
    "# and plot the data\n",
    "plt.scatter(x, y_obs,)\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(['Ridge Regression Fit', 'Truth',  'Data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not that great. We have a hyperparameter to play around with, the $\\lambda$ regularization constant. Below, we'll try a different value (in `scikit-learn` this parameter is called `alpha`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try lowering penality.\n",
    "ridge_model = Ridge(alpha = 0.01)\n",
    "ridge_model.fit(Phi, y_obs)\n",
    "theta_ridge = ridge_model.coef_\n",
    "\n",
    "# plot model\n",
    "y_model = Phi_plot @ theta_ridge\n",
    "plt.plot(x_plot, y_model, color = COLORS[1])\n",
    "\n",
    "# Also plot the truth\n",
    "plt.plot(x_plot, y_truth,  color = COLORS[3])\n",
    "\n",
    "# and plot the data\n",
    "plt.scatter(x, y_obs,)\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(['Ridge Regression Fit', 'Truth',  'Data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better, but maybe there's a better value for $\\lambda$ that we haven't tried out. Being a hyperparameter, $\\lambda$ is determined by validation, just like how the number of basis functions $n$ was determined by validation in a past example. Luckily, `scikit-learn` has this functionality built in in the `RidgeCV` class, which uses cross-validation to pick the best $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# Let's try lowering penality.\n",
    "ridge_model = RidgeCV(alphas = np.logspace(-6, 4))\n",
    "ridge_model.fit(Phi, y_obs)\n",
    "theta_ridge = ridge_model.coef_\n",
    "\n",
    "# plot model\n",
    "y_model = Phi_plot @ theta_ridge\n",
    "plt.plot(x_plot, y_model, color = COLORS[1])\n",
    "\n",
    "# Also plot the truth\n",
    "plt.plot(x_plot, y_truth,  color = COLORS[3])\n",
    "\n",
    "# and plot the data\n",
    "plt.scatter(x, y_obs,)\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(['Ridge Regression Fit', 'Truth',  'Data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso\n",
    "\n",
    "As discussed in the slides, L1 regularization (Lasso) takes a slightly different approach. It is used to identify **sparse** solutions - $\\vec \\theta$ values where most entries are 0. In this sense, this technique is good for **model selection**, wherein we have a large library of potentially relevant basis function/models, but we know the data represents a process in which only a small number of them are involved.\n",
    "\n",
    "Below, we'll consider a sinusoidal function with modes $\\sin(\\omega_i t)$, where the frequencies $\\omega_i$ can be one $\\left\\{1, 2, ..., 100\\right\\}$, but we may know that not all of these frequencies show up. For example, the true function could be\n",
    "\n",
    "$$ f(x) = 0.5 \\sin(x) + 0.2\\sin(33x) + 0.2\\sin(75x).$$\n",
    "\n",
    "Below we'll plot some noisy data we get from this truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The true function\n",
    "def f(x):\n",
    "    return 0.5*np.sin(x) + 0.2*np.sin(33.0*x) + 0.2*np.sin(75.0*x)\n",
    "\n",
    "# generate some data\n",
    "x_data = np.linspace(0, 4*np.pi, 100)\n",
    "y_data = f(x_data) + np.random.normal(0, 0.2, len(x_data))\n",
    "\n",
    "\n",
    "# generate truth over a more refined set just for plotting purposes\n",
    "x_plot = np.linspace(0, 4*np.pi, 200)\n",
    "y_plot = f(x_plot)\n",
    "\n",
    "# Plot truth and noisy observations\n",
    "plt.plot(x_plot, y_plot, color = COLORS[3])\n",
    "plt.scatter(x_data, y_data)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.legend(['Truth', 'Observations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to discover which are the relevant modes from the noisy data. We'll do this with Lasso, assuming the model\n",
    "\n",
    "$$ y = \\theta_1 \\sin(x) + \\theta_2 \\sin(2x) + ... + \\theta_{200}\\sin(200x).$$\n",
    "\n",
    "Before, when we were using the power series basis $\\left\\{1, x, x^2, ... , x^{n-1}\\right\\}$, the corresponding design matrix was called the Vandermonde matrix, and we had a special `numpy` function to build that for us called `numpy.vander`. For the Fourier basis $\\left\\{\\sin(x), \\sin(2x), ..., \\sin(nx)\\right\\}$, we'll have to make our own function to form the corresponding design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_fourier_design_matrix(X, n):\n",
    "    # number of rows = m = number of data points\n",
    "    num_rows = X.shape[0]\n",
    "    # number of columns = n = number of basis functions\n",
    "    num_cols = n\n",
    "    \n",
    "    # Allocate a m-by-n matrix of all zeros for now\n",
    "    Phi = np.zeros([num_rows, num_cols])\n",
    "    \n",
    "    # iterate over basis functions\n",
    "    for j in range(n):\n",
    "        # Apply the j-th basis function to the entire \n",
    "        # set of data and store as the j-th column of Phi\n",
    "        Phi[:,j] = np.sin((j+1)*X)      \n",
    "            \n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll do Lasso regression. In this context, we are pretending that we know that the truth is comprised of a small set of $sin(jx)$ functions, but we don't know which ones. We'll throw in the first 100 basis functions $\\sin(jx)$ and rely on the sparsity of our Lasso solution to help us pick out the small set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Lasso class from sklearn.linear_model\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# 1. Form the design matrix.\n",
    "NUM_BASIS = 100\n",
    "Phi = form_fourier_design_matrix(x_data, NUM_BASIS)\n",
    "\n",
    "# Perform the Lasso fit\n",
    "lasso_model = Lasso(alpha = 0.1, max_iter = 10000, fit_intercept=False)\n",
    "lasso_model.fit(Phi, y_data)\n",
    "\n",
    "# Get the Lasso parameters\n",
    "theta_lasso =  lasso_model.coef_\n",
    "\n",
    "# Do the same for ridge regression solution for comparison\n",
    "ridge_model = Ridge(alpha = 0.1, fit_intercept=False)\n",
    "ridge_model.fit(Phi, y_data)\n",
    "theta_ridge = ridge_model.coef_\n",
    "\n",
    "# Apply models by predicting the y values for the refined set of x values in x_plot\n",
    "Phi_plot = form_fourier_design_matrix(x_plot, NUM_BASIS)\n",
    "\n",
    "# Multiply design matrix by the thetas found in both lasso and ridge regression\n",
    "y_lasso = Phi_plot @ theta_lasso\n",
    "y_ridge = Phi_plot @ theta_ridge\n",
    "\n",
    "# Plot \n",
    "plt.plot(x_plot, y_lasso)\n",
    "plt.plot(x_plot, y_ridge, ls=':')\n",
    "plt.scatter(x_data, y_data, color = 'red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(['Lasso', 'Ridge regression', 'Observations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great, but let's see if we can identify which $sin(jx)$ contributed to the signal. To do that, we'll plot the $\\theta$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(theta_lasso)\n",
    "plt.plot(theta_ridge, ls=':')\n",
    "\n",
    "plt.xlabel('i')\n",
    "plt.ylabel('theta_i')\n",
    "plt.legend(['Lasso', 'Ridge'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the Lasso estimate for the $\\theta$ is quite sparse. Only about 5 out of 100 entries are non-zero. Remember, a relatively large non-zero value of $\\theta_j$ says that the regression believes $\\sin(jx)$ contributes to the signal (the observations). It doesn't appear that we're picking the correct $\\sin$ functions, which were:\n",
    "\n",
    "$$\\left\\{\\sin(x), \\sin(33x), \\sin(75x) \\right\\}.$$\n",
    "\n",
    "Maybe we didn't pick the $\\lambda$ correctly. Let's use the `LassoCV` to pick the best $\\lambda$ using cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "lasso_model = LassoCV(alphas = np.logspace(-3, 3))\n",
    "lasso_model.fit(Phi, y_data)\n",
    "theta_lasso =  lasso_model.coef_\n",
    "\n",
    "y_lasso = Phi_plot @ theta_lasso\n",
    "\n",
    "plt.plot(theta_lasso)\n",
    "plt.plot(theta_ridge, ls=':')\n",
    "\n",
    "plt.xlabel('i')\n",
    "plt.ylabel('theta_i')\n",
    "plt.legend(['Lasso', 'Ridge'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still not a great fit, and in reality, without extra information, this is perhaps the best we can hope to do. One extra piece of information, which is very common, is that we could assume that the $\\theta$ values are all positive, which we can force the `LassoCV` class to do by passing in the `positive = True` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model = LassoCV(alphas = np.logspace(-3, 3), positive = True)\n",
    "lasso_model.fit(Phi, y_data)\n",
    "theta_lasso =  lasso_model.coef_\n",
    "\n",
    "y_lasso = Phi_plot @ theta_lasso\n",
    "y_ridge = Phi_plot @ theta_ridge\n",
    "\n",
    "plt.plot(theta_lasso)\n",
    "plt.plot(theta_ridge, ls=':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that doing yields good results. Recall the original function was\n",
    "$$    y =  0.5\\sin(x) + 0.2\\sin(33x) + 0.2\\sin(75x) $$\n",
    "which as 3 modes at 1, 33, and 75. The three largest coefficient values in the Lasso estimate correspond exactly to the true $\\sin(jx)$ functions. The Lasso solution also predicts the presense of other function in the signal as well, but the magnitude of the corrsponding $\\theta$ values aren't as large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which modes?\n",
    "np.where(theta_lasso > 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
