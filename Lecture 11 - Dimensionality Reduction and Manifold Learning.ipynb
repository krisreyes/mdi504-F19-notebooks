{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----- make nice figures -----\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi']= 250\n",
    "\n",
    "from cycler import cycler\n",
    "COLORS = ['#553C67', '#F00D2C', '#4E8F00']\n",
    "default_cycler = cycler(color=COLORS)\n",
    "plt.rc('axes', axisbelow=True)\n",
    "plt.rc('axes', prop_cycle=default_cycler) \n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = np.loadtxt('data/perovskite_data.txt')\n",
    "\n",
    "# Class data - whether we form Perovskite or not\n",
    "y = data[:, 0]\n",
    "\n",
    "# Extract all but the 0-th column\n",
    "X = data[:, 1:]\n",
    "\n",
    "# Best to center your data when doing PCA\n",
    "X = (X - np.mean(X, axis=0))/np.std(X, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual SVD Calculation\n",
    "\n",
    "To understand what's going on under the hood, we'll explicitly calculate the singular vectors -- the \"natural\" basis of the data. Recall, the singluar vectors of the data matrix $X$ are the Eigenvectors of\n",
    "$$C = X^T X,$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SVD with eigvalue decomposition\n",
    "C = X.T@X\n",
    "\n",
    "# General eigenvector calculation\n",
    "Lambda_eig, U_eig = np.linalg.eig(C)\n",
    "\n",
    "# We just want the directions of the singular vectors, and not \n",
    "# necessarily their magnitude, so we can normalize all vectors\n",
    "# to have unit length.\n",
    "\n",
    "# Normalize columns of U_eig\n",
    "U_eig = U_eig / np.linalg.norm(U_eig, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot singular values\n",
    "Sigma_eig = np.sqrt(Lambda_eig)\n",
    "plt.plot(Sigma_eig)\n",
    "plt.xlabel('$i$')\n",
    "plt.ylabel('$\\sigma_i$')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's project our data onto the first largest two singular values\n",
    "X_proj_EIG = X@U_eig\n",
    "\n",
    "# Plot first two components\n",
    "plt.scatter(X_proj_EIG[:, 0], X_proj_EIG[:, 1], c=y)\n",
    "plt.xlabel('$u_1$')\n",
    "plt.ylabel('$u_2$')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD calculation using `numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use numpy's SVD library:\n",
    "V, Sigma_np, UT = np.linalg.svd(X, compute_uv=True)\n",
    "U = UT.T\n",
    "\n",
    "# Plot singular values\n",
    "plt.plot(Sigma_np)\n",
    "plt.xlabel('$i$')\n",
    "plt.ylabel('$\\sigma_i$')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's project our data onto the first two singular values\n",
    "X_proj_SVD = X@U\n",
    "\n",
    "# Plot first two components\n",
    "plt.scatter(X_proj_SVD[:, 0], X_proj_SVD[:, 1], c=y)\n",
    "plt.xlabel('$u_1$')\n",
    "plt.ylabel('$u_2$')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `Scikitlearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Scikit learn's PCA library\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a pca model \"object\"\n",
    "pca = PCA(n_components=10)\n",
    "\n",
    "# Train the model to the data\n",
    "pca.fit(X)\n",
    "\n",
    "# Do the dimensionality reduction\n",
    "X_proj_PCA = pca.transform(X)\n",
    "\n",
    "# Get singular values\n",
    "Sigma_PCA = pca.singular_values_ \n",
    "plt.plot(Sigma_PCA)\n",
    "plt.xlabel('$i$')\n",
    "plt.ylabel('$\\sigma_i$')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_proj_PCA[:, 0], X_proj_PCA[:, 1], c=y)\n",
    "plt.xlabel('$u_1$')\n",
    "plt.ylabel('$u_2$')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linear Manifold Learning\n",
    "\n",
    "When your data lies in some low-dimensional surface inside its ambient space, but that surface is not linear (i.e. \"flat\"), then methods like PCA fail to find this low dimensionality of your data.\n",
    "\n",
    "We have to turn to dimensionality reduction methods for non-flat structures -- this is called **Manifold Learning**.\n",
    "\n",
    "The `scikitlearn` library has many manifold learning methods available. For details see:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/manifold.html\n",
    "\n",
    "To illustrate the various methods, we'll load the \"swiss roll\" data set, which is data points lying inside a 3 dimensional space, but really essentially lying on a 2 dimensional surface of that space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some nonlinear data\n",
    "from sklearn import datasets\n",
    "\n",
    "# Make 3d plots\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "NUM_POINTS = 1000\n",
    "X, color = datasets.samples_generator.make_swiss_roll(NUM_POINTS)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[:,0], X[:,1], X[:,2], c = color)\n",
    "ax.view_init(4, -72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like the dimeninsionality reduction to \"capture\" this in 2D euclidean space -- i.e. the plane. That is, we're looking for dimensionality reduction algorithms to \"unwrap\" this and preserve structure as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear projections\n",
    "\n",
    "We'll try some linear projections, which won't be that great because the surface is non-linear. First, we'll just map to the $XY$ plane by forgetting each points' $Z$ coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot projetion onto plane z = 0\n",
    "plt.scatter(X[:,0], X[:,1], c=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try using PCA, again a linear projection (this time on the plane defined by the first 2 singular vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "Z = pca.transform(X)\n",
    "\n",
    "plt.scatter(Z[:, 0], Z[:, 1], c=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel PCA\n",
    "\n",
    "Recall the central idea of kernel PCA is to secretly define a high  non-linear transformation $\\phi(\\mathbf x)$ - with the hope that that transformation makes the dimensionality reduction easier. In `scikit-learn`, we can use the `KernelPCA` model to do this, and specify what kernel to use.\n",
    "\n",
    "Documentation:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "pca = KernelPCA(n_components=2, kernel='poly')\n",
    "pca.fit(X)\n",
    "Z = pca.transform(X)\n",
    "\n",
    "plt.scatter(Z[:, 0], Z[:, 1], c=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isomap\n",
    "\n",
    "Recall that Isomap works by trying to fit the points in a low dimensional Euclidean space so that geodesic distance is preserved to the best of our abilities. It approximates the geodesic distance by making a graph of points.\n",
    "\n",
    "The `Isomap` model in `scikit-learn` does this for your. Among other things, you must specify the number of neighbors in the graph that is built from the points (in `n_neighbors`) as well as the assumed dimension  of the manifold (in `n_components`).\n",
    "\n",
    "Documentation:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.manifold.Isomap.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "\n",
    "model = Isomap(n_neighbors = 10, n_components = 2)\n",
    "model.fit(X)\n",
    "Z = model.transform(X)\n",
    "\n",
    "plt.scatter(Z[:, 0], Z[:, 1], c=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other models\n",
    "\n",
    "`scikit-learn` has many other manifold learning models, and you can look at the documentation for a list of them. Below is just a quick example on a few others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Linear Embedding\n",
    "\n",
    "Documentation:\n",
    "    \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "model = LocallyLinearEmbedding(n_neighbors = 10, n_components = 2)\n",
    "model.fit(X)\n",
    "Z = model.transform(X)\n",
    "\n",
    "plt.scatter(Z[:, 0], Z[:, 1], c=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplacian Eigenmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import SpectralEmbedding\n",
    "\n",
    "model = SpectralEmbedding(n_components = 2, n_neighbors=10)\n",
    "Z = model.fit_transform(X)\n",
    "\n",
    "plt.scatter(Z[:, 0], Z[:, 1], c=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multidimensional Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "\n",
    "model = MDS(n_components = 2, metric = True)\n",
    "Z = model.fit_transform(X)\n",
    "\n",
    "plt.scatter(Z[:, 0], Z[:, 1], c=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-distributed Stochastic Neighbor Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "model = TSNE(n_components = 2)\n",
    "Z = model.fit_transform(X)\n",
    "\n",
    "plt.scatter(Z[:, 0], Z[:, 1], c=color)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
