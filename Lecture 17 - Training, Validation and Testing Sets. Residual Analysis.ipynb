{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----- make nice figures -----\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi']= 200\n",
    "from cycler import cycler\n",
    "COLORS = ['#F00D2C', '#242482', '#0071BE', '#4E8F00', '#553C67', '#DA5319']\n",
    "default_cycler = cycler(color=COLORS)\n",
    "plt.rc('axes', prop_cycle=default_cycler) \n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error vs. number of basis functions\n",
    "\n",
    "Given a good set of basis functions, it's actually quite easy to fit the data well -- even exactly at some point. But this isn't necessarily what we want. Below we'll explore this through the following example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = np.loadtxt('data/lec_17_example1.txt')\n",
    "x_data = data[:,0]\n",
    "y_data = data[:,1]\n",
    "\n",
    "# Plot the data\n",
    "plt.figure()\n",
    "plt.grid()\n",
    "plt.scatter(x_data, y_data)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try linear least-squares regression using the monomial basis, resulting in the truncated power series:\n",
    "\n",
    "$$ y = \\theta_0 + \\theta_1 x + \\theta_2 x^2 + ... + \\theta_n x^n, $$\n",
    "\n",
    "We'll define a function to do this for any given $n$, so we don't have to re-write the code. In the function, we\n",
    "have to define the design matrix given the monomial basis:\n",
    "\n",
    "$$ \\Phi = \\begin{pmatrix} 1 & x_1 & x_1^2 & \\cdots & x_1^n \\\\\n",
    "1 & x_2 & x_2^2 & \\cdots & x_2^n \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_m & x_m^2 & \\cdots & x_m^n\n",
    "\\end{pmatrix}.$$\n",
    "\n",
    "This design matrix has a name, the **Vandermonde Matrix**, which you can read about here:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Vandermonde_matrix\n",
    "\n",
    "We can use the special `numpy` function `np.vander` to form this matrix for our given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits a polynomial to the data using the monomial basis and linear least squares to find the coefficients \n",
    "def fit_poly(x, y, x_predict, n):\n",
    "    \n",
    "    # ---- TRAINING ----\n",
    "    # Form the design matrix.\n",
    "    Phi = np.vander(x, (n+1), increasing = True)\n",
    "    \n",
    "    # Get least squares solution\n",
    "    theta_ls = np.linalg.lstsq(Phi, y, rcond = None)[0]\n",
    "    \n",
    "    \n",
    "    # ---- PREDICTING ----\n",
    "    # predict the function at the points in x_predict\n",
    "    Phi_predict = np.vander(x_predict, (n+1), increasing = True)\n",
    "    y_predict = Phi_predict @ theta_ls\n",
    "    \n",
    "    return y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try to fit the best $n = 1$ polynomial, i.e. a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted function at each of these points\n",
    "x_plot = np.linspace(0, 1, 50)\n",
    "\n",
    "# The degree of the polynomial\n",
    "n = 1\n",
    "\n",
    "# perform least squares then predict\n",
    "y_plot = fit_poly(x_data, y_data, x_plot, n)\n",
    "\n",
    "# plot the data\n",
    "plt.plot(x_plot, y_plot, color = COLORS[1])\n",
    "plt.scatter(x_data, y_data)\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(['Model', 'Data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not that great of a fit. Let's calculate some measure of fit, such as the mean squared error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict at the data points themselves\n",
    "y_model = fit_poly(x_data, y_data, x_data, n)\n",
    "\n",
    "# calculate MSE\n",
    "MSE = np.mean((y_data - y_model)**2)\n",
    "\n",
    "print(\"MSE (n = \" + str(n) + \") = \" + str(MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try other values of $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "\n",
    "# perform least squares then predict\n",
    "y_plot = fit_poly(x_data, y_data, x_plot, n)\n",
    "\n",
    "# plot the data\n",
    "plt.plot(x_plot, y_plot, color = COLORS[1])\n",
    "plt.scatter(x_data, y_data)\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(['Model', 'Data'])\n",
    "\n",
    "# calculate MSE\n",
    "y_model = fit_poly(x_data, y_data, x_data, n)\n",
    "MSE = np.mean((y_data - y_model)**2)\n",
    "print(\"MSE (n = \" + str(n) + \") = \" + str(MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a very shallow parabola, as a result of trying to best fit the data. Try a few more by changing $n$ above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put this in a `for` loop, and calculate the MSE of the fit for various values of $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the MSE for several model sizes\n",
    "MSE_list = np.zeros(10)   \n",
    "for n in range(10):\n",
    "    # Train the model with n basis functions, predict the data\n",
    "    y_model = fit_poly(x_data, y_data, x_data, n)\n",
    "    # caculate the MSE\n",
    "    MSE_list[n] = np.mean((y_data - y_model)**2)    \n",
    "    \n",
    "# Plot MSE vs. degree\n",
    "plt.plot(MSE_list)\n",
    "plt.xlabel('n')\n",
    "plt.ylabel('MSE')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like MSE is minimized for large $n$. Does this mean we should take $n$ as large as possible? Let's look at the fit at $n = 20$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform least squares then predict\n",
    "y_plot = fit_poly(x_data, y_data, x_plot, 20)\n",
    "\n",
    "# plot the data\n",
    "plt.plot(x_plot, y_plot, color = COLORS[1])\n",
    "plt.scatter(x_data, y_data)\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.ylim([np.min(y_data), np.max(y_data)])\n",
    "plt.legend(['Model', 'Data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't look right. In fact, this is a classical case of **overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, Validation and Testing Sets\n",
    "\n",
    "How do we then pick the number of basis functions? (This number is called a **hyperparameter**). We'll pick it using a validation testing set. To do this, we'll need to break up our data into\n",
    "* Training data set (about 80% of the data)\n",
    "* Validation data set (about 80% of the remaining 20%)\n",
    "* Testing data set (the rest).\n",
    "\n",
    "These aren't set numbers, but generally rules of thumb. Below, for example, we'll split our 20 data points into sets of 12, 4 and 4 for the training, validation and testing sets, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = len(data)\n",
    "num_train = 12\n",
    "num_valid = 4\n",
    "\n",
    "# Since we're going to take contiguous subsets of data, it's best to shuffle \n",
    "# the data in case there was some inherent order in the data.\n",
    "I_perm = np.random.permutation(num_data)\n",
    "# rearrange the rows according to permutation\n",
    "data = data[I_perm, :]\n",
    "\n",
    "# rows 0-11 \n",
    "training_set = data[0:num_train, :]\n",
    "x_training = training_set[:, 0]\n",
    "y_training = training_set[:, 1]\n",
    "\n",
    "# rows 12-15\n",
    "validation_set = data[num_train:(num_train + num_valid), :]\n",
    "x_validation = validation_set[:, 0]\n",
    "y_validation = validation_set[:, 1]\n",
    "\n",
    "# rows 16 - 19\n",
    "testing_set = data[(num_train + num_valid):, :]\n",
    "x_testing = testing_set[:, 0]\n",
    "y_testing = testing_set[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30\n",
    "training_performance = np.zeros(N)\n",
    "validation_performance = np.zeros(N)\n",
    "\n",
    "# loop over each model\n",
    "for n in range(N):\n",
    "    # train model N (aka N basis elements) with training set\n",
    "    y_model = fit_poly(x_training, y_training, x_training, n)\n",
    "    training_performance[n] = np.mean((y_model - y_training)**2)\n",
    "    \n",
    "    # calculate performance with validation set\n",
    "    y_model = fit_poly(x_training, y_training, x_validation, n)\n",
    "    validation_performance[n] = np.mean((y_model - y_validation)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each model's performance\n",
    "plt.figure()\n",
    "plt.plot(training_performance)\n",
    "plt.plot(validation_performance)\n",
    "plt.xlabel('Model Size')\n",
    "plt.ylabel('Performance')\n",
    "plt.grid()\n",
    "plt.legend(['Training Performance', 'Validation Performance'])\n",
    "plt.yscale('log')\n",
    "\n",
    "# Pick best performing model\n",
    "n_best = np.argmin(validation_performance)\n",
    "print(\"Best model: n = \" + str(n_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now compare the final performance of the best model against testing set\n",
    "y_model = fit_poly(x_training, y_training, x_testing, n_best)     \n",
    "test_performance = np.mean((y_model - y_testing)**2)\n",
    "print(\"MSE for testing set: \" + str(test_performance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the best model, along with data sets\n",
    "x_plot = np.linspace(0, 1, 100)\n",
    "y_plot = y_model = fit_poly(x_training, y_training, x_plot, n_best)     \n",
    "\n",
    "# Plot the two models, along with data\n",
    "plt.plot(x_plot, y_plot, color = COLORS[1])\n",
    "plt.scatter(data[:,0], data[:,1])\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k fold cross-validation\n",
    "num_train = 16\n",
    "print(num_train)\n",
    "\n",
    "training_set = data[0:num_train, :]\n",
    "x_training = training_set[:, 0]\n",
    "y_training = training_set[:, 1]\n",
    "\n",
    "testing_set = data[num_train:, :]\n",
    "x_testing = testing_set[:, 0]\n",
    "y_testing = testing_set[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training data into 4 groups of 4\n",
    "k = 4\n",
    "num_per_group = int(num_train / k)\n",
    "\n",
    "# loop over models\n",
    "for n in range(N):\n",
    "\n",
    "    # perform k validations and save performance for each validation    \n",
    "    validation_performance_j = np.zeros(k)\n",
    "    \n",
    "    for j in range(k):\n",
    "        \n",
    "        # The boolean index to index the rows of training data that will be\n",
    "        # treated as validation data for this round\n",
    "        I = np.logical_and(j*num_per_group <= np.arange(num_train),\n",
    "                           np.arange(num_train) < (j+1)*num_per_group)\n",
    "\n",
    "        # Break up training data into this round's validation and training data\n",
    "        validation_set_j = training_set[I, :]\n",
    "        \n",
    "        # the training data is everything not in validation set\n",
    "        training_set_j = training_set[~I, :]\n",
    "        \n",
    "        # pick out input and output vectors from training set\n",
    "        x_training_j = training_set_j[:, 0]\n",
    "        y_training_j = training_set_j[:, 1]\n",
    "        \n",
    "        # pick out input and output vectors from validation set\n",
    "        x_validation_j = validation_set_j[:, 0]\n",
    "        y_validation_j = validation_set_j[:, 1]\n",
    "        \n",
    "        # train with training set\n",
    "        y_model = fit_poly(x_training_j, y_training_j, x_validation_j, n)\n",
    "        \n",
    "        # save performance for the j-th validation set\n",
    "        validation_performance_j[j] = np.mean((y_model -y_validation_j)**2)\n",
    "    \n",
    "        \n",
    "    # Overall model performance for the i-th model is \n",
    "    # just the average over all validation sets\n",
    "    validation_performance[n] = np.mean(validation_performance_j)\n",
    "    \n",
    "# At the end of this loop, model_performance will hold \n",
    "# the validation performance for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_performance)\n",
    "plt.plot(validation_performance)\n",
    "plt.xlabel('Model Size')\n",
    "plt.ylabel('Performance')\n",
    "plt.grid()\n",
    "plt.legend(['Training Performance', 'Validation Performance'])\n",
    "plt.yscale('log')\n",
    "\n",
    "# Pick best performing model\n",
    "n_best = np.argmin(validation_performance)\n",
    "print(\"Best model: n = \" + str(n_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now compare the final performance of the best model against testing set\n",
    "y_model = fit_poly(x_training, y_training, x_testing, n_best)     \n",
    "test_performance = np.mean((y_model - y_testing)**2)\n",
    "print(\"MSE for testing set: \" + str(test_performance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the best model, along with data sets\n",
    "x_plot = np.linspace(0, 1, 100)\n",
    "y_plot = y_model = fit_poly(x_training, y_training, x_plot, n_best)     \n",
    "\n",
    "# Plot the two models, along with data\n",
    "plt.plot(x_plot, y_plot, color = COLORS[1])\n",
    "plt.scatter(data[:,0], data[:,1])\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
