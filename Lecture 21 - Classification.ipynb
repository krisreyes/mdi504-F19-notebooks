{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----- make nice figures -----\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi']= 150\n",
    "from cycler import cycler\n",
    "COLORS = ['#F00D2C', '#242482', '#0071BE', '#4E8F00', '#553C67', '#DA5319', '#242482']\n",
    "default_cycler = cycler(color=COLORS)\n",
    "plt.rc('axes', prop_cycle=default_cycler) \n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Classification\n",
    "\n",
    "In this notebook, we take our first look at classification. We'll consider our first classification algorithm, called the **$k$-nearest neighbors algorithm**. We'll also think about how we measure the performance of our models using metrics such as **accuracy, precision and recall**.\n",
    "\n",
    "Below, we'll load and normalize some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from text\n",
    "data = np.loadtxt('data/lec21.txt')\n",
    "num_points = data.shape[0]\n",
    "\n",
    "# Shuffle the data to remove bias with respect to order\n",
    "I_perm = np.random.permutation(len(data))\n",
    "data = data[I_perm, :]\n",
    "\n",
    "# Normalized input data\n",
    "x = data[:, 0:2]\n",
    "x = (x - np.mean(x, axis = 0))/np.std(x, axis = 0)\n",
    "\n",
    "# The class data\n",
    "y = data[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For plotting purposes -- specifically to set the colors of each of the classes to red and blue, we'll create this list of color strings. Each entry is just a hexadecimal specification of a color for a corresponding data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn 0,1,2 to color strings just for plotting\n",
    "y_color = []\n",
    "for i in range(num_points):\n",
    "    y_color.append(COLORS[int(y[i])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we can plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x[:,0], x[:,1], c=y_color)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of classification is to predict the class for new data inputs, $x = (x_1, x_2)$, consistent with this data. Since this data is in 2 dimensions, and we can  see the data, and it's pretty obvious how to separate the data, this example seems somewhat contrived. However, the goal is to do this for higher-dimensional data, which we can't see and so we have to rely on the computer to do the classification for us.\n",
    "\n",
    "Again, as best practice, we should partion off train, validation and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = int(num_points*0.8*0.8)\n",
    "num_valid = int(num_points*0.8*0.2)\n",
    "\n",
    "x_train = x[:num_train]\n",
    "y_train = y[:num_train]\n",
    "    \n",
    "x_valid = x[num_train:num_train+num_valid]\n",
    "y_valid = y[num_train:num_train+num_valid]\n",
    "\n",
    "x_test = x[num_train+num_valid:]\n",
    "y_test = y[num_train+num_valid:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first classification model we'll look at is $k$-nearest neighbors (knn). To predict the class at any point $x = (x_1, x_2)$, knn looks for the $k$ closest training data points to $x$, and just takes a majority vote. The class with the highest vote is what we use to predict the class at $x$.\n",
    "\n",
    "`scikit-learn` has the $k$-nearest neighbors classifier in `sklearn.neighbors.KNeighborsClassifier`. We use it like any other `scikit-learn` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this model has been fit, we can use it to predict the class at any point $x = (x_1, x_2)$. Below, we'll color the entire plane according to this prediction. It's ok to ignore this part, but we'll use it to visualize our results. Basically, all we do is to generate a bunch of points $(x_1, x_2)$ (Lines 6-10), predict the class of these points according to the model (line 13), and use these predictions to color the whole plane (line 16-18)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Let's predict every point in the plane - you can ignore this part if you want\n",
    "\n",
    "# Form all combinations of points in the plane\n",
    "x1_plot = np.linspace(np.min(x[:,0])-0.1, np.max(x[:, 0])+0.1, 200)\n",
    "x2_plot = np.linspace(np.min(x[:,1])-0.1, np.max(x[:, 1])+0.1, 200)\n",
    "xx1, xx2 = np.meshgrid(x1_plot, x2_plot)\n",
    "# Flatten xx1 and xx2 to a list of points\n",
    "x_plot = np.array([xx1.ravel(), xx2.ravel()]).transpose()\n",
    "\n",
    "# classify each point\n",
    "y_plot = model.predict(x_plot)\n",
    "\n",
    "# shape into matrix so we can color the plane\n",
    "y_plot = y_plot.reshape(xx1.shape)\n",
    "# plot classification at each point as a colored region\n",
    "plt.pcolormesh(xx1, xx2, y_plot, cmap=ListedColormap(COLORS))\n",
    "\n",
    "# Plot the original data\n",
    "plt.scatter(x[:,0], x[:,1], marker='^', edgecolors='k', linewidth=0.75, c=y_color)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's be more quantatative with assessing the performance of the classifier. For this, we can use one of several metrics. First, we'll just see how well we predicted the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_model = model.predict(x_valid)\n",
    "print(y_valid_model)\n",
    "print(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll calculate Accuracy, Precision and Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(y_valid, y_valid_model)\n",
    "print(\"Accuracy = \" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "M = confusion_matrix(y_valid, y_valid_model)\n",
    "print(M)\n",
    "print(\"\")\n",
    "print(\"TN  = \" + str(M[0,0]))\n",
    "print(\"FN  = \" + str(M[1,0]))\n",
    "print(\"TP  = \" + str(M[1,1]))\n",
    "print(\"FP  = \" + str(M[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision = precision_score(y_valid, y_valid_model)\n",
    "recall = recall_score(y_valid, y_valid_model)\n",
    "\n",
    "print(\"Precision = \" + str(precision))\n",
    "print(\"Recall = \" + str(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "f1_score = fbeta_score(y_valid, y_valid_model, beta = 1)\n",
    "print(\"F1 Score = \" + str(f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Below we'll use logistic regression, which maps inputs to binary classes\n",
    "\n",
    "$$ c = \\text{sgn}(\\varphi(\\mathbf x) \\cdot \\theta).$$\n",
    "\n",
    "where $\\text{sgn}$ is the sign function and $\\varphi(\\mathbf x)$ is the vector of the basis function values evaluated at an input $\\mathbf x$.\n",
    "\n",
    "We need some basis functions that take in two inputs $x_1, x_2$. We'll use the monomial basis to form all degree $d$ monomials in 2 variables, and evaluate these monomials on our data to form a design matrix. Before, when we only had one input variable, we could form the analogous design matrix using the `np.vander` function, but in higher dimensions, we have to write our own. (Soon we won't have to write our own when we once again turn to kernel functions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import scipy\n",
    "\n",
    "# multivariate analog of np.vander - forms monomial basis design matrix\n",
    "def form_design_matrix(X, k = 1):\n",
    "    num_rows = X.shape[0]\n",
    "    num_cols = X.shape[1]\n",
    "    \n",
    "    # Add a column of 1's to the set of variables\n",
    "    XX = np.concatenate((np.ones([num_rows, 1]), X), axis=1)\n",
    "    \n",
    "    # form the list of all multivariable monomials with total degree = 1\n",
    "    combinations = itertools.combinations_with_replacement(range(num_cols+1), k)\n",
    "    # the number of such monomials\n",
    "    num_combinations = int(scipy.special.binom(num_cols+1 + k - 1, k))\n",
    "    Phi_T = np.zeros([num_combinations, num_rows])\n",
    "    \n",
    "    # iterate over all monomials\n",
    "    for j, exp in enumerate(combinations):\n",
    "        # iterate over all data points\n",
    "        for i in range(num_rows):\n",
    "            # apply the j-th monomial\n",
    "            Phi_T[j, i] = 1\n",
    "            for l in range(k):\n",
    "                Phi_T[j, i] = Phi_T[j, i]*XX[i, exp[l]]\n",
    "            Phi_T[j,i] = Phi_T[j,i]\n",
    "\n",
    "    Phi = np.transpose(Phi_T)\n",
    "    \n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "\n",
    "Phi_train = form_design_matrix(x_train, n)\n",
    "Phi_valid = form_design_matrix(x_valid, n)\n",
    "\n",
    "print(Phi_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we can form design matrices, we can use the logistic regression model in `sklearn.linear_model.LogisticRegression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(fit_intercept = False)\n",
    "model.fit(Phi_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_model = model.predict(Phi_valid)\n",
    "\n",
    "acc = accuracy_score(y_valid, y_valid_model)\n",
    "precision = precision_score(y_valid, y_valid_model)\n",
    "recall = recall_score(y_valid, y_valid_model)\n",
    "f1_score = fbeta_score(y_valid, y_valid_model, beta = 1)\n",
    "\n",
    "print(\"Accuracy = \" + str(acc))\n",
    "print(\"Precision = \" + str(precision))\n",
    "print(\"Recall = \" + str(recall))\n",
    "print(\"F1 Score = \" + str(f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_model = [1 for _ in range(len(x_valid))]\n",
    "\n",
    "acc = accuracy_score(y_valid, y_valid_model)\n",
    "precision = precision_score(y_valid, y_valid_model)\n",
    "recall = recall_score(y_valid, y_valid_model)\n",
    "f1_score = fbeta_score(y_valid, y_valid_model, beta = 1)\n",
    "\n",
    "print(\"Accuracy = \" + str(acc))\n",
    "print(\"Precision = \" + str(precision))\n",
    "print(\"Recall = \" + str(recall))\n",
    "print(\"F1 Score = \" + str(f1_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
