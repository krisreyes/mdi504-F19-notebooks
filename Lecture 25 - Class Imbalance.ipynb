{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----- make nice figures -----\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi']= 150\n",
    "\n",
    "from cycler import cycler\n",
    "COLORS = ['#242482', '#F00D2C', '#242482', '#0071BE', '#4E8F00', '#553C67', '#DA5319', '#F00D2C']\n",
    "default_cycler = cycler(color=COLORS)\n",
    "plt.rc('axes', prop_cycle=default_cycler) \n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from text\n",
    "data = np.loadtxt('data/lec25.txt')\n",
    "\n",
    "num_points = data.shape[0]\n",
    "\n",
    "# Shuffle the data to remove bias with respect to order\n",
    "I_perm = np.random.permutation(len(data))\n",
    "data = data[I_perm, :]\n",
    "\n",
    "# Extract input and outputs\n",
    "x = data[:, 0:2]\n",
    "\n",
    "# The class data\n",
    "y = data[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn 0,1 to color strings just for plotting\n",
    "y_color = []\n",
    "for i in range(num_points):\n",
    "    y_color.append(COLORS[int(y[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.dpi']= 200\n",
    "plt.scatter(x[:,0], x[:,1], c=y_color)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = np.bincount(y.astype(int))\n",
    "print(\"Counts:\")\n",
    "print(class_counts)\n",
    "\n",
    "print(\"\\nPercentages:\")\n",
    "print(100.0*class_counts/len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Predict every point in the plane - you can ignore this part if you want\n",
    "def plot_plane(model):\n",
    "    # Form all combinations of points in the plane\n",
    "    x1_plot = np.linspace(np.min(x[:,0])-0.1, np.max(x[:, 0])+0.1, 200)\n",
    "    x2_plot = np.linspace(np.min(x[:,1])-0.1, np.max(x[:, 1])+0.1, 200)\n",
    "    xx1, xx2 = np.meshgrid(x1_plot, x2_plot)\n",
    "    # Flatten xx1 and xx2 to a list of points\n",
    "    x_plot = np.array([xx1.ravel(), xx2.ravel()]).transpose()\n",
    "\n",
    "    # classify each point\n",
    "    y_plot = model.predict(x_plot)\n",
    "\n",
    "    # shape into matrix so we can color the plane\n",
    "    y_plot = y_plot.reshape(xx1.shape)\n",
    "    # plot classification at each point as a colored region\n",
    "    plt.pcolormesh(xx1, xx2, y_plot, cmap=ListedColormap(COLORS))\n",
    "\n",
    "    # Plot the original data\n",
    "    plt.scatter(x[:,0], x[:,1], marker='^', edgecolors='k', linewidth=0.75, c=y_color)\n",
    "    plt.xlabel('X1')\n",
    "    plt.ylabel('X2')\n",
    "    \n",
    "def report_performance(model, x_train, y_train):\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    y_valid_model = model.predict(x_valid)\n",
    "    plot_plane(model)\n",
    "    print(\" Accuracy = \" + str(accuracy_score(y_valid, y_valid_model)))\n",
    "    print(\"Precision = \" + str(precision_score(y_valid, y_valid_model)))\n",
    "    print(\"   Recall = \" + str(recall_score(y_valid, y_valid_model)))\n",
    "    print(\"       F1 = \" + str(f1_score(y_valid, y_valid_model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results ignoring class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "num_train = int(num_points*0.8*0.8)\n",
    "num_valid = int(num_points*0.8*0.2)\n",
    "\n",
    "# Extract\n",
    "x_train = x[:num_train]\n",
    "y_train = y[:num_train]\n",
    "    \n",
    "x_valid = x[num_train:num_train+num_valid]\n",
    "y_valid = y[num_train:num_train+num_valid]\n",
    "\n",
    "x_test = x[num_train+num_valid:]\n",
    "y_test = y[num_train+num_valid:]\n",
    "\n",
    "# Normalize data\n",
    "# IMPORTANT: Normalization parameters must be derived from training data\n",
    "mu_x = np.mean(x_train, axis = 0)\n",
    "sig_x = np.std(x_train, axis = 0)\n",
    "\n",
    "x_train = (x_train - mu_x)/sig_x\n",
    "x_valid = (x_valid - mu_x)/sig_x\n",
    "x_test = (x_test - mu_x)/sig_x\n",
    "x = (x - mu_x)/sig_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "report_performance(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "report_performance(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel='rbf')\n",
    "report_performance(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(criterion='entropy')\n",
    "report_performance(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reweigh data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel='rbf', class_weight = {0: 0.3*num_train, 1: 0.7*num_train})\n",
    "report_performance(model, x_train, y_train)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class0_weights = np.linspace(0.1, 1, 10)\n",
    "\n",
    "for w in class0_weights:\n",
    "    model = SVC(kernel='rbf', class_weight = {0: w*num_train, 1: (1-w)*num_train})\n",
    "    print(\"WEIGHT = \" + str(w))\n",
    "    report_performance(model, x_train, y_train)    \n",
    "    plt.title(\"Class 0 weight = \" + str(100.0*w) + \"%\" )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can optimize this, or use the \"balanced\" weighting as a good rule of thumb. This weights each point inversely proportional to the size of the class the point belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel='rbf', class_weight = 'balanced')\n",
    "report_performance(model, x_train, y_train)\n",
    "plt.title(\"Class 0 weight = \" + str(100.0*model.class_weight_[0]) + \"%\" )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same goes for the Logistic Regression Model and classification trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(class_weight = 'balanced')\n",
    "report_performance(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(criterion='entropy', class_weight = 'balanced')\n",
    "report_performance(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's no change the the classification tree performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Over and undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate training data into classes\n",
    "\n",
    "I_0 = y_train == 0\n",
    "I_1 = y_train == 1\n",
    "\n",
    "x_train_0 = x_train[I_0, :]\n",
    "x_train_1 = x_train[I_1, :]\n",
    "\n",
    "y_train_0 = y_train[I_0] #(all 0s)\n",
    "y_train_1 = y_train[I_1] #(all 1s)\n",
    "\n",
    "n0 = len(y_train_0)\n",
    "n1 = len(y_train_1)\n",
    "\n",
    "print(\"Size of class 0 in training data: \" + str(n0))\n",
    "print(\"Size of class 0 in training data: \" + str(n1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersample majority class - select only n1 for class 0 (randomly, without replacement) 88 -> 31\n",
    "I_train_0 = np.random.choice(n0, n1, replace = False)\n",
    "x_train_0_undersample = x_train_0[I_train_0, :]\n",
    "y_train_0_undersample = y_train_0[I_train_0] #(all 0s)\n",
    "\n",
    "# Put the data back together: all the class 1 data and the undersampled class 0 data\n",
    "x_train_undersample = np.concatenate([x_train_0_undersample, x_train_1])\n",
    "y_train_undersample = np.concatenate([y_train_0_undersample, y_train_1])\n",
    "print(x_train_undersample.shape)\n",
    "print(y_train_undersample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validate some models\n",
    "model = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "report_performance(model, x_train_undersample, y_train_undersample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "report_performance(model, x_train_undersample, y_train_undersample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel='rbf')\n",
    "report_performance(model, x_train_undersample, y_train_undersample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(criterion='entropy')\n",
    "report_performance(model, x_train_undersample, y_train_undersample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversample: sample n0 samples for class 1 (with replacement) 31 -> 88\n",
    "\n",
    "I_train_1 = np.random.choice(n1, n0, replace = True)\n",
    "print(I_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_1_oversample = x_train_1[I_train_1, :]\n",
    "y_train_1_oversample = y_train_1[I_train_1] #(all 1s)\n",
    "\n",
    "# Put the data back together: all the class 0 data and the oversampled class 0 data\n",
    "x_train_oversample = np.concatenate([x_train_1_oversample, x_train_0])\n",
    "y_train_oversample = np.concatenate([y_train_1_oversample, y_train_0])\n",
    "print(x_train_oversample.shape)\n",
    "print(y_train_oversample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train some models\n",
    "model = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "report_performance(model, x_train_oversample, y_train_oversample)\n",
    "plt.title('KNN')\n",
    "plt.show()\n",
    "\n",
    "model = LogisticRegression()\n",
    "report_performance(model, x_train_oversample, y_train_oversample)\n",
    "plt.title('Logistic Regression')\n",
    "plt.show()\n",
    "\n",
    "model = SVC(kernel='rbf')\n",
    "report_performance(model, x_train_oversample, y_train_oversample)\n",
    "plt.title('SVM')\n",
    "plt.show()\n",
    "\n",
    "model = DecisionTreeClassifier(criterion='entropy')\n",
    "report_performance(model, x_train_oversample, y_train_oversample)\n",
    "plt.title('Classification_tree')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data\n",
    "\n",
    "We'll use the SMOTE oversampling technique built into `imbalanced-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE()\n",
    "x_train_smote, y_train_smote = sm.fit_resample(x_train, y_train)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_train_smote.shape)\n",
    "\n",
    "# how is the imbalance?\n",
    "print(\"Resampled 0 class size = \" + str(np.sum(y_train_smote == 0)))\n",
    "print(\"Resampled 1 class size = \" + str(np.sum(y_train_smote == 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train some models\n",
    "model = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "report_performance(model, x_train_smote, y_train_smote)\n",
    "plt.title('KNN')\n",
    "plt.show()\n",
    "\n",
    "model = LogisticRegression()\n",
    "report_performance(model, x_train_smote, y_train_smote)\n",
    "plt.title('Logistic Regression')\n",
    "plt.show()\n",
    "\n",
    "model = SVC(kernel='rbf')\n",
    "report_performance(model, x_train_smote, y_train_smote)\n",
    "plt.title('SVM')\n",
    "plt.show()\n",
    "\n",
    "model = DecisionTreeClassifier(criterion='entropy')\n",
    "report_performance(model, x_train_smote, y_train_smote)\n",
    "plt.title('Classification tree')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other oversampling / undersampling techniques are implemented in `imbalanced-learn`:\n",
    "    \n",
    "https://imbalanced-learn.readthedocs.io/en/stable/user_guide.html\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
