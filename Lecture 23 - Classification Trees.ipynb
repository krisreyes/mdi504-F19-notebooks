{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----- make nice figures -----\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi']= 200\n",
    "from cycler import cycler\n",
    "COLORS = ['#F00D2C', '#242482', '#0071BE', '#4E8F00', '#553C67', '#DA5319']\n",
    "default_cycler = cycler(color=COLORS)\n",
    "plt.rc('axes', prop_cycle=default_cycler) \n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('data/perovskite_data.txt')\n",
    "\n",
    "# Shuffle data\n",
    "I_perm = np.random.permutation(len(data))\n",
    "data = data[I_perm, :]\n",
    "\n",
    "# Class data - whether we form Perovskite or not\n",
    "c = data[:, 0]\n",
    "\n",
    "# Extract all but the 0-th column\n",
    "X = data[:, 1:]\n",
    "\n",
    "# Normalize data\n",
    "X = (X - np.mean(X, axis=0))/np.std(X, axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training, validation and testing set\n",
    "num_points = len(c)\n",
    "num_train = int(0.8*0.8*num_points)\n",
    "num_valid = int(0.8*0.2*num_points)\n",
    "num_test = len(c) - num_train - num_valid\n",
    "\n",
    "X_train = X[:num_train, :]\n",
    "c_train = c[:num_train]\n",
    "\n",
    "X_valid = X[num_train:(num_train + num_valid), :]\n",
    "c_valid = c[num_train:(num_train + num_valid)]\n",
    "\n",
    "# get the last \"num_test\" rows\n",
    "X_test = X[-num_test:, :]\n",
    "c_test = c[-num_test:]\n",
    "\n",
    "print(\"         Total data size: \" + str(num_points))\n",
    "print(\"  Training data set size: \" + str(num_train))\n",
    "print(\"Validation data set size: \" + str(num_valid))\n",
    "print(\"   Testing data set size: \" + str(num_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC(kernel='poly', degree = 4, gamma = 'auto')\n",
    "svm_model.fit(X_train, c_train)\n",
    "c_model = svm_model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = f1_score(c_model, c_valid)\n",
    "print(\"score \" + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_vals = np.logspace(-2, 2)\n",
    "f1_scores = np.zeros(len(C_vals))\n",
    "\n",
    "for i, C in enumerate(C_vals):\n",
    "    svm_model = SVC(kernel='poly', degree = 4, gamma = 'auto', C = C)\n",
    "    svm_model.fit(X_train, c_train)\n",
    "    c_model = svm_model.predict(X_valid)\n",
    "    f1_scores[i] = f1_score(c_model, c_valid)\n",
    "\n",
    "plt.plot(C_vals, f1_scores)\n",
    "plt.grid()\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Regularization parameter C')\n",
    "plt.ylabel('F1 Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_best = np.argmax(f1_scores)\n",
    "C_best = C_vals[i_best]\n",
    "\n",
    "# train final model\n",
    "svm_model = SVC(kernel='poly', degree = 4, gamma = 'auto', C = C_best)\n",
    "svm_model.fit(X_train, c_train)\n",
    "\n",
    "# Predict accuracy on test data\n",
    "c_model = svm_model.predict(X_test)\n",
    "accuracy = accuracy_score(c_model, c_test)\n",
    "f1 = f1_score(c_model, c_test)\n",
    "print(\"Estimted accuracy for new data = \" + str(100.0*accuracy) + \"%\")\n",
    "print(\"Estimted f1 score for new data = \" + str(100.0*f1) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Trees\n",
    "\n",
    "Documentation:\n",
    "http://www.scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification tree model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# train with training data\n",
    "model = DecisionTreeClassifier(criterion='entropy')\n",
    "model.fit(X_train, c_train)\n",
    "\n",
    "c_model = model.predict(X_valid)\n",
    "score = accuracy_score(c_model, c_valid)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a different \"uniformity measure\"\n",
    "model = DecisionTreeClassifier(criterion='gini')\n",
    "model.fit(X_train, c_train)\n",
    "c_model = model.predict(X_valid)\n",
    "score = accuracy_score(c_model, c_valid)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vary \"max_depth\"\n",
    "depths = np.arange(1, 30)\n",
    "scores = np.zeros(len(depths))\n",
    "\n",
    "for i, d in enumerate(depths):\n",
    "    model = DecisionTreeClassifier(criterion='gini', max_depth = d)\n",
    "    model.fit(X_train, c_train)\n",
    "    c_model = model.predict(X_valid)\n",
    "    scores[i] = accuracy_score(c_model, c_valid)\n",
    "\n",
    "plt.plot(depths, scores)\n",
    "plt.xlabel('Max tree depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict accuracy on test data\n",
    "model = DecisionTreeClassifier(criterion='gini', max_depth = 6)\n",
    "model.fit(X_train, c_train)\n",
    "c_model = model.predict(X_test)\n",
    "accuracy = accuracy_score(c_model, c_test)\n",
    "f1 = f1_score(c_model, c_test)\n",
    "print(\"Estimted accuracy for new data = \" + str(100.0*accuracy) + \"%\")\n",
    "print(\"Estimted f1 score for new data = \" + str(100.0*f1) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "plot_tree(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"A-EN - O-EN\", \"rA/rO\", \"A-ionicity\", \"dA-O\", \"B-EN - O-EN\", \"rB/rO\", \"B-ionicity\", \"dB-O\", \"dA-O/dB-O\", \"A-EN - B-EN\", \"rA/rB\", \"t_BV\", \"t_IR\", \"GII\"]\n",
    "plot_tree(model, feature_names = feature_names )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = model.feature_importances_\n",
    "print(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = [2*i for i in range(len(importance))]\n",
    "plt.barh(x_plot, model.feature_importances_)\n",
    "plt.yticks(x_plot, feature_names)\n",
    "plt.grid(axis = 'x')\n",
    "plt.xlabel('Importance')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
