{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----- make nice figures -----\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi']= 100\n",
    "from cycler import cycler\n",
    "COLORS = ['#F00D2C', '#242482', '#0071BE', '#4E8F00', '#553C67', '#DA5319']\n",
    "default_cycler = cycler(color=COLORS)\n",
    "plt.rc('axes', prop_cycle=default_cycler) \n",
    "# -----------------------------\n",
    "\n",
    "# This will be used to make the 3d plots interactive\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of the linear model\n",
    "\n",
    "For linear regression, we use the linear model\n",
    "$$ y(x) = \\theta_1 \\varphi_1(x) + \\theta_2 \\varphi_2(x) + ... + \\theta_n \\varphi_n(x),$$\n",
    "\n",
    "The functions $\\varphi_j(x)$ are called **basis functions**, and they have to be specified before we can\n",
    "run linear regression. Choosing the best basis functions for your given data is a kind of art. Below, we'll explore a few simple examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "Here, our choice of basis functions are:\n",
    "* $ \\mathbf x = (x_1, x_2)$\n",
    "* $ \\varphi_1(x_1, x_2) = x_1$\n",
    "* $ \\varphi_2(x_1, x_2) = x_2$\n",
    "\n",
    "This results in the model:\n",
    "$$ y = \\theta_1 x_1 + \\theta_2 x_2 $$\n",
    "\n",
    "Let's plot this for a choice of the parameters $\\theta = (\\theta_1, \\theta_2)^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_1 = -1\n",
    "theta_2 = 0.5\n",
    "\n",
    "# We'll plot the model function in the interval [-1, 1] for both input variables\n",
    "x1 = np.linspace(-1, 1, 10)\n",
    "x2 = np.linspace(-1, 1, 10)\n",
    "\n",
    "# Form all pairs of points (x_1, x_2)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "\n",
    "# Evaluate the function at all the point\n",
    "y = theta_1*X1 + theta_2*X2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have 2 inputs and one output, to plot $y$ versus $x_1$ and $x_2$, we'll make 3D surface plot. To plot the surface, we need to use `matplotlib`'s 3D functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we do this, the `matplotlib` axis `ax` can make 3D plots. We'll use the `plot_surface` function. See the documentation of `Axis3D` to see what other kinds of 3D plots you can make.\n",
    "\n",
    "https://matplotlib.org/3.1.1/api/_as_gen/mpl_toolkits.mplot3d.axes3d.Axes3D.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the surface plot\n",
    "ax.plot_surface(X1, X2, y)\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "Here we consider a different set of basis functions. It's important to note that basis functions can be whatever you want/need them to be, if you think the function represents your data somehow. Below, we picked basis functions that are polynomials in one or both input variables. We also picked a constant function $\\varphi_3(x) = 1$. This allows us to have an \"intercept\" term.\n",
    "\n",
    "* $ \\varphi_1(x_1, x_2) = x^2_2$\n",
    "* $ \\varphi_2(x_1, x_2) = x^2_1 + x_2$\n",
    "* $ \\varphi_3(x_1, x_2) = 1$\n",
    "\n",
    "This results in the model function\n",
    "\n",
    "$$ y = \\theta_1 x^2_2 + \\theta_2(x^2_1 + x_2) + \\theta_3$$\n",
    "\n",
    "We see that without the constant term $\\varphi_3(x) = 1$, we would not have the parameter $\\theta_3$ in the above. This parameter is an example of an \"intercept\" term. The model predicts that at $x_1 = x_2 = 0$, $y = \\theta_3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_1 = 2\n",
    "theta_2 = -1\n",
    "theta_3 = 10\n",
    "\n",
    "# Evaluate the function\n",
    "y = theta_1*X2**2 + theta_2*(X1**2 + X2) + theta_3\n",
    "\n",
    "# Plot the function\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(X1, X2, y)\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Least Squares Examples\n",
    "\n",
    "Given some data points $\\left\\{(\\textbf x_i, y_i)\\right\\}_{i=1}^m$, and a linear model \n",
    "$$y = f(\\textbf x,  \\theta)$$\n",
    "$$ = \\theta_1 \\varphi_2(\\textbf x) + \\theta_2 \\varphi_2(\\textbf x) + ... + \\theta_n \\varphi_n(\\textbf x),$$\n",
    "**linear least squares regression** attempts to find the value of parameters $\\theta$ that minimize the cost function\n",
    "\n",
    "$$J(\\theta) = \\sum_{i=1}^m \\left[ y_i - f(\\textbf x_i, \\theta) \\right]^2. $$\n",
    "\n",
    "After some calculus, we can show that the parameter values, $\\hat \\theta_\\text{LS}$, that minimize the above\n",
    "cost function satisfies the **normal equations**\n",
    "\n",
    "$$ \\Phi^T \\mathbf y = \\Phi^T\\Phi \\theta, $$\n",
    "\n",
    "where $\\Phi$ is the design matrix, with elements $\\Phi_{i,j} = \\varphi_j(\\mathbf x_i).$$ By solving this linear system, we get the optimal parameter values. Below we'll see several examples of how this is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop making interactive plots\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.dpi']= 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the data in `data/lec_14_ex1.txt`, which has 2 columns corresponding to a 1 dimensional input variable $x$ and the output variable $y$. There are several $(x, y)$ pairs in this data file. Below, we plot this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.loadtxt('data/lec_15_ex1.txt', delimiter =',')\n",
    "x = X[:,0]\n",
    "y = X[:,1]\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data suggests that the output is a polynomial function of the input, so as a basis, we'll use the monomial basis: $\\left\\{1, x, x^2, x^3\\right\\}$. Once we do this, we have to evaluate each of our data points on each of the basis functions to form the design matrix $\\Phi$. Recall that the rows of $\\Phi$ correspond to each of the basis elements evaluated at a single input value $x_i$. Columns of $\\Phi$ correspond to a single basis function, evaluated at every input value $x_i$ in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate basis functions for columns of the design matrix\n",
    "phi_0 = np.ones(len(x))\n",
    "phi_1 = x**1\n",
    "phi_2 = x**2\n",
    "phi_3 = x**3\n",
    "\n",
    "# Design matrix\n",
    "Phi = np.array([phi_0, phi_1, phi_2, phi_3]).T\n",
    "print(Phi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we form the design matrix, we can form the **Moore-Penrose pseudoinverse**:\n",
    "\n",
    "$$\\Phi^+ = (\\Phi^T\\Phi)\\Phi^T.$$\n",
    "\n",
    "We'll use the matrix multiplication operator `@`, which can be used on `numpy` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Moore-Penrose inverse\n",
    "Phi_plus = np.linalg.inv(Phi.T @ Phi) @ Phi.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the pseudoinverse, we can get the least squares solution\n",
    "\n",
    "$$\\hat \\theta_\\text{LS} = \\Phi^+ \\mathbf y.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve for least square solution\n",
    "theta_LS = Phi_plus @ y\n",
    "print(theta_LS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with the basis functions, the parameter values $\\hat \\theta_\\text{LS}$ yield a fully-specified model, that\n",
    "we can evaluate at any $x$ to predict a value of $y$.\n",
    "\n",
    "Below, we'll evaluate the model at 50 uniformly spaced points $x_i$ between -1 and 1. We'll also plot the data to compare the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have a model to evaluate the function at any x\n",
    "x_plot = np.linspace(-1, 1, 50)\n",
    "y_plot =  theta_LS[0] + theta_LS[1]*x_plot + theta_LS[2]*x_plot**2 + theta_LS[3]*x_plot**3\n",
    "\n",
    "# Plot the model and the data\n",
    "plt.plot(x_plot, y_plot, color = COLORS[1])\n",
    "plt.scatter(x, y)\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(['Model', 'Data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2\n",
    "\n",
    "This example uses the Fourier basis instead of the monomial basis. Other than that, everything else is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and plot the data\n",
    "X = np.loadtxt('data/lec_15_ex2.txt', delimiter =',')\n",
    "x = X[:,0]\n",
    "y = X[:,1]\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(x, y, s = 7)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eavaluate basis functions and form design matrix\n",
    "phi_0 = np.ones(len(x))\n",
    "phi_1 = np.sin(x)\n",
    "phi_2 = np.sin(2*x)\n",
    "phi_3 = np.sin(3*x)\n",
    "phi_4 = np.sin(4*x)\n",
    "\n",
    "Phi = np.array([phi_0, phi_1, phi_2, phi_3, phi_4]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Moore-Penrose inverse\n",
    "Phi_plus = np.linalg.inv(Phi.T @ Phi) @ Phi.T\n",
    "\n",
    "# Solve for least square solution\n",
    "theta_LS = Phi_plus @ y\n",
    "\n",
    "print(theta_LS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = np.linspace(-6, 6, 100)\n",
    "\n",
    "# Evaluate y_plot using a for-loop\n",
    "y_plot = theta_LS[0]*np.ones(len(x_plot))\n",
    "for i in range(1, len(theta_LS)):\n",
    "    y_plot = y_plot + theta_LS[i]*np.sin(i*x_plot)\n",
    "\n",
    "# Plot the model and the data\n",
    "plt.plot(x_plot, y_plot, color = COLORS[1])\n",
    "plt.scatter(x, y)\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(['Model', 'Data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `numpy` functions for least-squares solutions\n",
    "\n",
    "In fact, using the Moore-Penrose pseuodinverse explicitly is not that great. One of the reasons is that it necessitates the explict calculation of an inverse $(\\Phi^T\\Phi)^{-1}$. In practice, you really should never calculate an inverse of a matrix. (See https://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/). It is computationally expensive, and often numerically unstable.\n",
    "\n",
    "How, then, do we solve the normal equations\n",
    "\n",
    "$$ \\Phi^T \\mathbf y = \\Phi^T\\Phi \\theta\\text{ ?}$$ \n",
    "\n",
    "In general, there are ways of solving the system $b = A \\theta$ without inverting $A$. (In our example, $b = \\Phi^T \\mathbf y$, and $A = \\Phi^T \\Phi$.) How this is done is slightly out of the scope of this course. Instead, we'll point you to functions that solve this system the right way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First is using np.linalg.solve to solve the normal equations\n",
    "theta = np.linalg.solve(Phi.T @ Phi, Phi.T @ y)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can additionally use np.linalg.lstsq to compute the least squares solution to the original equation\n",
    "theta = np.linalg.lstsq(Phi, y, rcond = None)[0]\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use the LinearRegression class in scikit-learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Instantiate a model\n",
    "model = LinearRegression(fit_intercept = False)\n",
    "\n",
    "# train model\n",
    "model.fit(Phi, y)\n",
    "\n",
    "# get the thet avalues\n",
    "theta = model.coef_\n",
    "print(theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict at any x value, but we have to evaluate the \n",
    "# x values at the basis functions, i.e. form the\n",
    "# design matrix for the points we wish to predict\n",
    "phi0_plot = np.ones(len(x_plot))\n",
    "phi1_plot = np.sin(x_plot)\n",
    "phi2_plot = np.sin(2*x_plot)\n",
    "phi3_plot = np.sin(3*x_plot)\n",
    "phi4_plot = np.sin(4*x_plot)\n",
    "Phi_plot =  np.array([phi0_plot, phi1_plot, phi2_plot, phi3_plot, phi4_plot]).T\n",
    "\n",
    "# Predict the response \n",
    "y_plot = model.predict(Phi_plot)\n",
    "\n",
    "# Plot the model and the data\n",
    "plt.plot(x_plot, y_plot, color = COLORS[1])\n",
    "plt.scatter(x, y)\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(['Model', 'Data'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
